{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following file cleans the data (using classes defined in the MergeCleanClasses file), splits the data into train and test sets, and merges in variables generated from the \"Chief Complaint\" variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data relevant to our research question is in the following three files. \n",
    "#Note these contain background information about patients (age, sex, etc), as well as information obtained at the paitent's arrival at the ED. \n",
    "# Although there is other data available, it is obtained later in the patient's ED stay.\n",
    "ed_edstays_df = pd.read_csv('../ed_data/edstays.csv.gz', compression='gzip')\n",
    "ed_triage_df = pd.read_csv('../ed_data/triage.csv.gz', compression='gzip') \n",
    "hosp_patients_df = pd.read_csv('../hosp_data/patients.csv.gz', compression='gzip')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the three dataframes edstays and triage\n",
    "from MergeCleanClasses import DataMerge \n",
    "df_merge = DataMerge(ed_edstays_df, ed_triage_df, hosp_patients_df).get_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataframe by replacing abnormal values with np.nan in multiple columns, including\n",
    "#   stay_length_hours, temperature, resprate, sbp, dbp, o2sat, pain\n",
    "# Special treatments for several columns:\n",
    "# 1. For temperature, scale temperature values by 10 (if between 8.24 and 10.10) and by 0.10 (if between 824 and 1010),\n",
    "#      and convert temperature from Celsius to Fahrenheit (if betweeen 28 and 43.3)\n",
    "# 2. Exclude data where disposition is 'eloped', 'left without being seen' or 'left against medical advice'\n",
    "# 3. For pain, \n",
    "#   if it's a range (number-number), replace this range by the average (e.g., 6-9 is replaced by 7.5);\n",
    "#   if it's a number followed by a symbol, strip off the extra symbols and just keep the number  (e.g., '9+' is replaced by 9);\n",
    "#   it's a single number, convert to a float (covers integer and decimal). Round down to 10 if the result is larger than 10.\n",
    "# 4. Condense race into fewer categories: 'White', 'OTHER', 'HISPANIC/LATINO', 'ASIAN', 'BLACK', nan,\n",
    "#    'NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER', 'AMERICAN INDIAN/ALASKA NATIVE'\n",
    "# All above are copied from Emilie's Cleaning.ipynb\n",
    "\n",
    "# Additional cleaning I made:\n",
    "# 1. Change stay_length_hours to stay_length_minutes and drop those stay_length_minutes < 1 or > 10000\n",
    "# 2. Set 'heartrate' to NaN if greater than 400\n",
    "# 3. Set 'resprate' to NaN if greater than 100 or less than 20\n",
    "# 4. Scale sbp values up by 10 (if between 5 and 30) and down by 0.10 (if between 500 and 3000)\n",
    "#    and set 'sbp' to NaN if greater than 300 or less than 50\n",
    "# 5. Scale dbp values up by 10 (if between 2 and 20) and down by 0.10 (if between 200 and 2000)\n",
    "#    and set 'dbp' to NaN if greater than 200 or less than 20\n",
    "# 6. Set O2sat to NaN if greater than 100 or less than 20 \n",
    "# 7. For chief complaint, drop those rows where chief complaints do not contain any letters. Those are considered as null values such as ['___', '.', '-', nan, '___ ___', '\\\\', '\\x81\\x80\\x87', '\\x88\\x87', '1', '0', '\\x87', '?', '353', '2', '21', '___ /___', '___, ___', '150', '? ___', '\\x89', '___,___', '--']\n",
    "\n",
    "from MergeCleanClasses import DataCleaner\n",
    "cleaner=DataCleaner(df_merge)\n",
    "cleaner.clean_data()\n",
    "df_cleaned = cleaner.get_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>admission_age</th>\n",
       "      <th>temperature</th>\n",
       "      <th>heartrate</th>\n",
       "      <th>resprate</th>\n",
       "      <th>o2sat</th>\n",
       "      <th>sbp</th>\n",
       "      <th>dbp</th>\n",
       "      <th>acuity</th>\n",
       "      <th>stay_length_minutes</th>\n",
       "      <th>pain_cleaned_advanced</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.099730e+05</td>\n",
       "      <td>2.009690e+05</td>\n",
       "      <td>4.099730e+05</td>\n",
       "      <td>409901.000000</td>\n",
       "      <td>386970.000000</td>\n",
       "      <td>393218.000000</td>\n",
       "      <td>59869.000000</td>\n",
       "      <td>389703.000000</td>\n",
       "      <td>392068.000000</td>\n",
       "      <td>391218.000000</td>\n",
       "      <td>403114.000000</td>\n",
       "      <td>409973.000000</td>\n",
       "      <td>383558.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.500700e+07</td>\n",
       "      <td>2.499450e+07</td>\n",
       "      <td>3.499759e+07</td>\n",
       "      <td>53.077289</td>\n",
       "      <td>98.087712</td>\n",
       "      <td>85.050677</td>\n",
       "      <td>21.391438</td>\n",
       "      <td>98.389827</td>\n",
       "      <td>135.029382</td>\n",
       "      <td>77.397434</td>\n",
       "      <td>2.617880</td>\n",
       "      <td>435.226365</td>\n",
       "      <td>4.332826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.880169e+06</td>\n",
       "      <td>2.888970e+06</td>\n",
       "      <td>2.887985e+06</td>\n",
       "      <td>20.612036</td>\n",
       "      <td>0.978956</td>\n",
       "      <td>17.723182</td>\n",
       "      <td>3.194889</td>\n",
       "      <td>2.075818</td>\n",
       "      <td>22.388631</td>\n",
       "      <td>14.775016</td>\n",
       "      <td>0.709048</td>\n",
       "      <td>393.844532</td>\n",
       "      <td>3.817120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000003e+07</td>\n",
       "      <td>2.000002e+07</td>\n",
       "      <td>3.000001e+07</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>82.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.251441e+07</td>\n",
       "      <td>2.248891e+07</td>\n",
       "      <td>3.249910e+07</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>97.600000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>217.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.501340e+07</td>\n",
       "      <td>2.499833e+07</td>\n",
       "      <td>3.499260e+07</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>332.300000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.750095e+07</td>\n",
       "      <td>2.749324e+07</td>\n",
       "      <td>3.750435e+07</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>98.600000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.999999e+07</td>\n",
       "      <td>2.999981e+07</td>\n",
       "      <td>3.999996e+07</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9677.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         subject_id       hadm_id       stay_id  admission_age    temperature  \\\n",
       "count  4.099730e+05  2.009690e+05  4.099730e+05  409901.000000  386970.000000   \n",
       "mean   1.500700e+07  2.499450e+07  3.499759e+07      53.077289      98.087712   \n",
       "std    2.880169e+06  2.888970e+06  2.887985e+06      20.612036       0.978956   \n",
       "min    1.000003e+07  2.000002e+07  3.000001e+07      18.000000      82.400000   \n",
       "25%    1.251441e+07  2.248891e+07  3.249910e+07      35.000000      97.600000   \n",
       "50%    1.501340e+07  2.499833e+07  3.499260e+07      54.000000      98.000000   \n",
       "75%    1.750095e+07  2.749324e+07  3.750435e+07      69.000000      98.600000   \n",
       "max    1.999999e+07  2.999981e+07  3.999996e+07      91.000000     110.000000   \n",
       "\n",
       "           heartrate      resprate          o2sat            sbp  \\\n",
       "count  393218.000000  59869.000000  389703.000000  392068.000000   \n",
       "mean       85.050677     21.391438      98.389827     135.029382   \n",
       "std        17.723182      3.194889       2.075818      22.388631   \n",
       "min         1.000000     20.000000      42.000000      50.000000   \n",
       "25%        72.000000     20.000000      97.000000     120.000000   \n",
       "50%        84.000000     20.000000      99.000000     133.000000   \n",
       "75%        96.000000     22.000000     100.000000     148.000000   \n",
       "max       256.000000    100.000000     100.000000     299.000000   \n",
       "\n",
       "                 dbp         acuity  stay_length_minutes  \\\n",
       "count  391218.000000  403114.000000        409973.000000   \n",
       "mean       77.397434       2.617880           435.226365   \n",
       "std        14.775016       0.709048           393.844532   \n",
       "min        20.000000       1.000000             1.000000   \n",
       "25%        68.000000       2.000000           217.000000   \n",
       "50%        77.000000       3.000000           332.300000   \n",
       "75%        87.000000       3.000000           504.000000   \n",
       "max       199.000000       5.000000          9677.000000   \n",
       "\n",
       "       pain_cleaned_advanced  \n",
       "count          383558.000000  \n",
       "mean                4.332826  \n",
       "std                 3.817120  \n",
       "min                 0.000000  \n",
       "25%                 0.000000  \n",
       "50%                 5.000000  \n",
       "75%                 8.000000  \n",
       "max                10.000000  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test= train_test_split(df_cleaned, shuffle = True, random_state = 489,test_size=.1)\n",
    "\n",
    "#train_prelim=train_prelim.reset_index()\n",
    "#test_prelim=test_prelim.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d5/3vmjgsdd5tx_pz5vmfxtrtrc0000gn/T/ipykernel_91656/2676777163.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[column_labels]=train_pca\n",
      "/var/folders/d5/3vmjgsdd5tx_pz5vmfxtrtrc0000gn/T/ipykernel_91656/2676777163.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[column_labels]=train_pca\n",
      "/var/folders/d5/3vmjgsdd5tx_pz5vmfxtrtrc0000gn/T/ipykernel_91656/2676777163.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[column_labels]=train_pca\n",
      "/var/folders/d5/3vmjgsdd5tx_pz5vmfxtrtrc0000gn/T/ipykernel_91656/2676777163.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train[column_labels]=train_pca\n",
      "/var/folders/d5/3vmjgsdd5tx_pz5vmfxtrtrc0000gn/T/ipykernel_91656/2676777163.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test[column_labels]=test_pca\n",
      "/var/folders/d5/3vmjgsdd5tx_pz5vmfxtrtrc0000gn/T/ipykernel_91656/2676777163.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test[column_labels]=test_pca\n",
      "/var/folders/d5/3vmjgsdd5tx_pz5vmfxtrtrc0000gn/T/ipykernel_91656/2676777163.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test[column_labels]=test_pca\n",
      "/var/folders/d5/3vmjgsdd5tx_pz5vmfxtrtrc0000gn/T/ipykernel_91656/2676777163.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test[column_labels]=test_pca\n"
     ]
    }
   ],
   "source": [
    "#Load the PCA results and add as new columns to the train and test dataframes.\n",
    "train_pca=np.load('../chief_complaint_data/train_pca.npy')\n",
    "column_labels = [f'cc_{i}' for i in range(100)]\n",
    "train[column_labels]=train_pca\n",
    "\n",
    "test_pca=np.load('../chief_complaint_data/test_pca.npy')\n",
    "test[column_labels]=test_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These csv files are now ready for modeling.\n",
    "train.to_csv('train.csv', index=False)\n",
    "test.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edpredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
