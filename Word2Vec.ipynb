{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score, root_mean_squared_error\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from scipy.stats import permutation_test, f_oneway\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('DataCleaning/train.csv')\n",
    "chief_complaint = train[['chiefcomplaint', 'stay_length_minutes']]\n",
    "train = train.drop(columns = ['subject_id', 'hadm_id', 'stay_id', 'race', 'pain', 'intime', 'outtime', 'chiefcomplaint'])\n",
    "\n",
    "train['race_condensed'] = train['race_condensed'].fillna('Missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data (chief complaints and corresponding treatment times)\n",
    "data = pd.DataFrame({\n",
    "    \"chief_complaint\": [\"Chest pain\", \"Abd pain\", \"Dyspnea\", \"SI\", \"s/p Fall\"],\n",
    "    \"treatment_time\": [30, 40, 35, 50, 25]  # Treatment times in minutes\n",
    "})\n",
    "\n",
    "# Tokenize complaints\n",
    "data[\"tokenized_complaint\"] = data[\"chief_complaint\"].str.lower().str.split()\n",
    "\n",
    "# Train a Word2Vec model on all complaints\n",
    "word2vec_model = Word2Vec(sentences=data[\"tokenized_complaint\"], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Custom Transformer for Word2Vec Embeddings\n",
    "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.vector_size = model.vector_size\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # No fitting needed\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([self._get_sentence_embedding(words) for words in X])\n",
    "\n",
    "    def _get_sentence_embedding(self, words):\n",
    "        vectors = [self.model.wv[word] for word in words if word in self.model.wv]\n",
    "        return np.mean(vectors, axis=0) if vectors else np.zeros(self.vector_size)\n",
    "\n",
    "# Pipeline for processing text features\n",
    "text_pipeline = FeatureUnion([\n",
    "    (\"tfidf\", TfidfVectorizer()),  # TF-IDF feature extraction\n",
    "    (\"word2vec\", Word2VecTransformer(model=word2vec_model))  # Word2Vec feature extraction\n",
    "])\n",
    "\n",
    "# Full pipeline: Preprocessing + Model Training\n",
    "pipeline = Pipeline([\n",
    "    (\"features\", text_pipeline),\n",
    "    (\"model\", RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"chief_complaint\"], data[\"treatment_time\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"Predictions:\", y_pred)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
